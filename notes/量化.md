## 量化

### 量化的任务

量化本质上只是对数值范围的重新调整，可以“粗略”的理解是一种线性映射。

<img src="https://raw.githubusercontent.com/nashpan/image-hosting/main/image-20250905101931431.png" alt="image-20250905101931431" style="zoom:67%;" />



量化任务的总结：

1、[量化映射](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=量化映射&zhida_source=entity)方法，也就是将float-32映射到Int数据类型，每个间隔是相等的还是不相等的，这里就是[均匀量化](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=均匀量化&zhida_source=entity)(uniform quantization)和[非均匀量化](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=非均匀量化&zhida_source=entity)(non-uniform quantization)，也可以叫作线性量化和非线性量化

2、关于映射到整数是数值范围是有正负数，还是都是正数，这里就是[对称量化](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=对称量化&zhida_source=entity)(有正负数)和[非对称量化](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=非对称量化&zhida_source=entity)(全是正数)，非对称量化就有zero-point，zero-point的主要作用是用于做padding。

3、原精度即浮float-32，量化到什么样的数据类型，这里就有float和int；到底要选择量化后的是多少个bit，这里就有1-bit(二值网络)、2-bit(三值网络)、3-bit、4-bit、5-bit、6-bit、7-bit、8-bit，这几种量化后的数值类型是整型。

4、是固定所有网络都是相同的bit-width，还是不同的，这里就有[混合精度量化](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=混合精度量化&zhida_source=entity)(Mixed precision)

5、是从一个已经训练好的模型再进行量化，还是有fine tune的过程或者直接是从头开始训练一个量化的模型，这里就有[Post-training quantization](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=Post-training+quantization&zhida_source=entity)(后量化，即将已经训练完的模型参数进行量化)、[quantization-aware training](https://zhida.zhihu.com/search?content_id=165836307&content_type=Article&match_order=1&q=quantization-aware+training&zhida_source=entity)(量化感知训练，即在从头开始训练中加入量化)和quantization-aware fine tune(在fine tune训练中加入量化)。



### 量化的误差

量化的误差来自于哪里？

- 从float-32到Int数据类型，其中有一个round的操作，这里肯定会有误差；
- 激活函数的截断；
- 溢出时候的处理也有可能带来误差。



### 大模型量化

模型量化的对象主要包括以下几个方面：

- 权重(weight)：weight的量化是最常规也是最常见的。量化weight可达到减少模型大小内存和占用空间。
- 激活(activation)：实际中activation往往是占用内存的大头，因此量化activation不仅可以大大减少内存占用。更重要的是，结合weight的量化可以充分利用整数计算获得性能提升。
- KV cache：量化KV缓存对于提高长序列生成的吞吐量至关重要。

按照量化参数s和z的**共享范围**,量化方法可以分为逐层量化、逐通道和逐组量化



#### LLM.int8()

> https://arxiv.org/pdf/2208.07339

原理：是一种采用混合精度分解的量化方法。该方案先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（vector-wise）。对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。

> 背景信息：作者发现激活中存在一些离群值，它们的绝对值明显更大；并且这些离群值分布在少量的几个特征中，称为离群特征 (Emergent Features)。这些离群特征会影响量化过程，因此先将这些值找出来，对于非异常值，正常int8量化，

<img src="https://raw.githubusercontent.com/nashpan/image-hosting/main/image-20250905110645135.png" alt="image-20250905110645135" style="zoom: 67%;" />

这种方法量化的性能下降较小，**但是这种分离计算的方式拖慢了推理速度。**(对于小模型，速度差距更加明显)

目前，LLM.int8() 的实现主要在 `bitsandbytes` 库，这个已经集成到了transformers库中

4bit量化

```python
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```



#### GPTQ

仅权重量化的方案

使用GPTQ： [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ)(也集成到了transformers库中)



#### SmoothQuant

激活值比权重更难量化，因为权重数据分布一般比较均匀，而激活异常值多且大让激活值量化变得更艰难，但是异常值只存在少数通道内。

因此，SmoothQuant诞生。SmoothQuant通过平滑激活层和权重后，再使用per-tensor或per-token量化，实现W8A8。与其他量化方法相比，该方法可以保持较高的精度，同时，具有更低的延迟。



#### AWQ

背景：该方法源于“权重对于LLM的性能并不同等重要”的观察，存在约（0.1%-1%）显著权重对大模型性能影响太大，通过跳过这1%的显著权重（salient weight）不进行量化，可以大大减少量化误差。

<img src="https://raw.githubusercontent.com/nashpan/image-hosting/main/image-20250905113839931.png" alt="image-20250905113839931" style="zoom: 60%;" />

生态：

[AutoAWQ](https://link.zhihu.com/?target=https%3A//github.com/casper-hansen/AutoAWQ)、[vLLM](https://link.zhihu.com/?target=https%3A//github.com/vllm-project/vllm/blob/main/vllm/model_executor/quantization_utils/awq.py)、 [HuggingFace TGI](https://link.zhihu.com/?target=https%3A//github.com/huggingface/text-generation-inference/pull/1054)、[LMDeploy](https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy)、NVIDIA [TensorRT-LLM](https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT-LLM/)、[FastChat](https://link.zhihu.com/?target=https%3A//github.com/lm-sys/FastChat/blob/main/docs/awq.md) 均有对AWQ的支持，transformers库也集成了AutoAWQ





### 参考资料

1. [*深度学习*中的模型*量化*入门](https://zhuanlan.zhihu.com/p/1918276482612073256)
2. [*量化* | 浅谈*深度学习*模型量化](https://zhuanlan.zhihu.com/p/349678095)
3. **墙裂推荐看这个专栏就足够了**！！！ [神经网络量化入门--基本原理](https://zhuanlan.zhihu.com/p/149659607)
4. [量化感知训练（Quantization-aware-training）探索-从原理到实践](https://zhuanlan.zhihu.com/p/548174416)
5. [深度学习模型压缩方法（三）：量化](https://zhuanlan.zhihu.com/p/619914824)